"""
Dataset Adapter Integration Guide
Complete workflow for processing any fraud dataset with your trained IEEE model.
"""

import pandas as pd
import numpy as np
from pathlib import Path
import sys
from typing import Tuple, Dict, Any

# Add project root to path
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from app.ml.dataset_adapter import SmartDatasetAdapter, adapt_fraud_dataset
# Assuming you have these modules ready:
# from app.ml.preprocessing_advanced import create_advanced_features
# from app.ml.predictor import create_predictor


class FraudDetectionPipeline:
    """
    Complete pipeline that combines dataset adaptation, feature engineering, 
    and fraud prediction for any input dataset.
    """
    
    def __init__(self, model_path: str = None, verbose: bool = True):
        """
        Initialize the complete fraud detection pipeline.
        
        Args:
            model_path: Path to trained IEEE model
            verbose: Whether to print progress
        """
        self.verbose = verbose
        self.model_path = model_path
        self.adapter = SmartDatasetAdapter(verbose=verbose)
        self.predictor = None
        self.feature_engineer = None
        
        # Load model if path provided
        if model_path:
            self._load_model()
    
    def _load_model(self):
        """Load the trained fraud detection model."""
        try:
            # This would load your trained model
            # self.predictor = create_predictor(self.model_path, verbose=self.verbose)
            if self.verbose:
                print(f"‚úÖ Model loaded from {self.model_path}")
        except Exception as e:
            if self.verbose:
                print(f"‚ö†Ô∏è  Could not load model: {e}")
    
    def process_dataset(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Complete pipeline: adapt dataset ‚Üí engineer features ‚Üí predict fraud.
        
        Args:
            df: Input fraud dataset in any format
            
        Returns:
            Dictionary with results and metadata
        """
        if self.verbose:
            print("üöÄ Starting Complete Fraud Detection Pipeline")
            print("=" * 50)
        
        results = {
            'success': False,
            'original_shape': df.shape,
            'error': None
        }
        
        try:
            # Step 1: Adapt dataset to IEEE format
            if self.verbose:
                print("Step 1: Adapting dataset to IEEE format...")
            
            adaptation_result = self.adapter.adapt_dataset(df)
            adapted_df = adaptation_result.adapted_data
            
            results.update({
                'adaptation_result': adaptation_result,
                'adapted_shape': adapted_df.shape,
                'dataset_type': adaptation_result.dataset_type.value,
                'adaptation_confidence': adaptation_result.adaptation_confidence
            })
            
            # Step 2: Feature engineering (if available)
            if self.verbose:
                print("Step 2: Advanced feature engineering...")
            
            try:
                # This would use your feature engineering pipeline
                # train_features, _, engineer = create_advanced_features(
                #     adapted_df, verbose=self.verbose
                # )
                # results['engineered_features'] = train_features
                # results['feature_count'] = train_features.shape[1]
                
                # For demonstration, we'll simulate this
                results['feature_count'] = 627  # Your IEEE model expects 627 features
                if self.verbose:
                    print(f"‚úÖ Feature engineering complete: {results['feature_count']} features")
                
            except Exception as e:
                if self.verbose:
                    print(f"‚ö†Ô∏è  Feature engineering failed: {e}")
                results['feature_engineering_error'] = str(e)
            
            # Step 3: Fraud prediction (if model available)
            if self.verbose:
                print("Step 3: Fraud prediction...")
            
            if self.predictor:
                # This would use your predictor
                # predictions = self.predictor.predict_batch(train_features)
                # results['predictions'] = predictions
                pass
            else:
                # Simulate predictions for demonstration
                n_samples = len(adapted_df)
                simulated_predictions = {
                    'fraud_probabilities': np.random.random(n_samples) * 0.1,  # Low fraud rates
                    'risk_scores': np.random.randint(1, 100, n_samples),
                    'predictions': np.random.choice([0, 1], n_samples, p=[0.95, 0.05])
                }
                results['predictions'] = simulated_predictions
                
                if self.verbose:
                    fraud_count = simulated_predictions['predictions'].sum()
                    fraud_rate = fraud_count / n_samples
                    print(f"‚úÖ Predictions complete: {fraud_count} fraud cases ({fraud_rate:.1%})")
            
            # Step 4: Generate analytics
            results.update(self._generate_analytics(adapted_df, results.get('predictions')))
            
            results['success'] = True
            
            if self.verbose:
                print("üéâ Pipeline completed successfully!")
            
        except Exception as e:
            results['error'] = str(e)
            if self.verbose:
                print(f"‚ùå Pipeline failed: {e}")
        
        return results
    
    def _generate_analytics(self, df: pd.DataFrame, predictions: Dict[str, Any]) -> Dict[str, Any]:
        """Generate analytics and insights from the results."""
        analytics = {}
        
        if predictions:
            analytics.update({
                'total_transactions': len(df),
                'fraud_count': int(predictions['predictions'].sum()),
                'fraud_rate': float(predictions['predictions'].mean()),
                'avg_risk_score': float(np.mean(predictions['risk_scores'])),
                'high_risk_count': int((predictions['risk_scores'] > 70).sum())
            })
        
        # Dataset characteristics
        if 'TransactionAmt' in df.columns:
            analytics.update({
                'avg_transaction_amount': float(df['TransactionAmt'].mean()),
                'max_transaction_amount': float(df['TransactionAmt'].max()),
                'transaction_amount_std': float(df['TransactionAmt'].std())
            })
        
        return analytics


def demo_streamlit_integration():
    """
    Demonstrate how to integrate the adapter with Streamlit.
    This shows the code you'd use in your Streamlit app.
    """
    
    streamlit_code = '''
import streamlit as st
import pandas as pd
from app.ml.dataset_adapter import adapt_fraud_dataset
from app.ml.preprocessing_advanced import create_advanced_features
from app.ml.predictor import create_predictor

def main_upload_interface():
    st.title("üîç Fraud Detection Analysis")
    st.subheader("Upload your dataset for instant fraud analysis")
    
    # File uploader
    uploaded_file = st.file_uploader(
        "Choose a CSV or Excel file",
        type=['csv', 'xlsx', 'xls'],
        help="Upload any transaction dataset with amount, time, and ID columns"
    )
    
    if uploaded_file:
        # Load the dataset
        try:
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            
            st.success(f"Dataset loaded successfully! Shape: {df.shape}")
            
            # Show original data preview
            st.subheader("Original Data Preview")
            st.dataframe(df.head())
            
            # Adapt dataset
            with st.spinner("Adapting dataset to IEEE format..."):
                adaptation_result = adapt_fraud_dataset(df, verbose=False)
            
            # Show adaptation results
            st.subheader("Dataset Adaptation Results")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Dataset Type", adaptation_result.dataset_type.value)
            with col2:
                st.metric("Adaptation Confidence", f"{adaptation_result.adaptation_confidence:.1%}")
            with col3:
                st.metric("Final Shape", f"{adaptation_result.adapted_data.shape[0]} √ó {adaptation_result.adapted_data.shape[1]}")
            
            # Feature engineering
            with st.spinner("Performing advanced feature engineering..."):
                train_features, _, engineer = create_advanced_features(
                    adaptation_result.adapted_data, verbose=False
                )
            
            st.success(f"Feature engineering complete! Generated {train_features.shape[1]} features")
            
            # Load model and predict
            @st.cache_resource
            def load_model():
                return create_predictor("models/saved_models", verbose=False)
            
            predictor = load_model()
            
            with st.spinner("Predicting fraud..."):
                predictions = predictor.predict_batch(train_features, include_shap=False)
            
            # Display results
            display_fraud_results(predictions, adaptation_result.adapted_data)
            
        except Exception as e:
            st.error(f"Error processing file: {e}")


def display_fraud_results(predictions, original_data):
    """Display fraud detection results in Streamlit."""
    
    # Extract metrics
    fraud_probs = [p.fraud_probability for p in predictions]
    risk_scores = [p.risk_score for p in predictions]
    fraud_predictions = [1 if p.fraud_probability > 0.5 else 0 for p in predictions]
    
    fraud_count = sum(fraud_predictions)
    fraud_rate = fraud_count / len(predictions)
    avg_risk = np.mean(risk_scores)
    
    # Overview metrics
    st.subheader("Fraud Detection Results")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Transactions", len(predictions))
    with col2:
        st.metric("Fraud Detected", fraud_count)
    with col3:
        st.metric("Fraud Rate", f"{fraud_rate:.2%}")
    with col4:
        st.metric("Avg Risk Score", f"{avg_risk:.1f}/100")
    
    # Risk distribution chart
    import plotly.express as px
    
    risk_df = pd.DataFrame({
        'Risk Score': risk_scores,
        'Fraud Probability': fraud_probs
    })
    
    fig = px.histogram(risk_df, x='Risk Score', title='Risk Score Distribution')
    st.plotly_chart(fig, use_container_width=True)
    
    # High-risk transactions table
    if fraud_count > 0:
        st.subheader("High-Risk Transactions")
        
        high_risk_indices = [i for i, p in enumerate(predictions) if p.fraud_probability > 0.5]
        high_risk_data = original_data.iloc[high_risk_indices].copy()
        high_risk_data['Fraud_Probability'] = [predictions[i].fraud_probability for i in high_risk_indices]
        high_risk_data['Risk_Score'] = [predictions[i].risk_score for i in high_risk_indices]
        
        st.dataframe(high_risk_data.head(20))
    
    # Download results
    results_df = original_data.copy()
    results_df['Fraud_Probability'] = fraud_probs
    results_df['Risk_Score'] = risk_scores
    results_df['Fraud_Prediction'] = fraud_predictions
    
    csv = results_df.to_csv(index=False)
    st.download_button(
        label="Download Results as CSV",
        data=csv,
        file_name="fraud_detection_results.csv",
        mime="text/csv"
    )

if __name__ == "__main__":
    main_upload_interface()
'''
    
    print("Streamlit Integration Code:")
    print("=" * 50)
    print(streamlit_code)


def test_end_to_end_workflow():
    """Test the complete end-to-end workflow."""
    print("üß™ Testing End-to-End Fraud Detection Workflow")
    print("=" * 60)
    
    # Create sample data
    sample_data = {
        'transaction_id': [f'TXN_{i:06d}' for i in range(100)],
        'amount': np.random.lognormal(mean=3, sigma=1, size=100),
        'timestamp': pd.date_range('2023-01-01', periods=100, freq='H'),
        'merchant_type': np.random.choice(['grocery', 'gas', 'online'], 100),
        'card_number': np.random.randint(1000, 9999, 100)
    }
    
    df = pd.DataFrame(sample_data)
    
    print(f"Sample Dataset Shape: {df.shape}")
    print("Sample Data:")
    print(df.head())
    
    # Initialize pipeline
    pipeline = FraudDetectionPipeline(verbose=True)
    
    # Process dataset
    results = pipeline.process_dataset(df)
    
    # Display results
    print("\n" + "=" * 40)
    print("PIPELINE RESULTS")
    print("=" * 40)
    
    if results['success']:
        print("‚úÖ Pipeline completed successfully!")
        print(f"   Original shape: {results['original_shape']}")
        print(f"   Adapted shape: {results['adapted_shape']}")
        print(f"   Dataset type: {results['dataset_type']}")
        print(f"   Adaptation confidence: {results['adaptation_confidence']:.1%}")
        print(f"   Feature count: {results['feature_count']}")
        
        if 'predictions' in results:
            analytics = results
            print(f"   Total transactions: {analytics['total_transactions']}")
            print(f"   Fraud detected: {analytics['fraud_count']}")
            print(f"   Fraud rate: {analytics['fraud_rate']:.1%}")
            print(f"   Average risk score: {analytics['avg_risk_score']:.1f}")
    else:
        print(f"‚ùå Pipeline failed: {results['error']}")
    
    return results


if __name__ == "__main__":
    print("Dataset Adapter Integration Guide")
    print("=" * 50)
    
    # Test the complete workflow
    results = test_end_to_end_workflow()
    
    print("\n" + "=" * 60)
    print("INTEGRATION EXAMPLES")
    print("=" * 60)
    
    # Show Streamlit integration
    demo_streamlit_integration()
    
    print("\nüéØ Next Steps:")
    print("1. Save the dataset adapter as: app/ml/dataset_adapter.py")
    print("2. Update your Streamlit app to use the adapter")
    print("3. Test with real fraud datasets")
    print("4. Deploy and enjoy cross-dataset support!")